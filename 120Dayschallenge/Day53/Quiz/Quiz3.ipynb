{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the main idea behind the Random Forest Regressor?**\n",
    "\n",
    "- Combining multiple weak models to create a stronger model\n",
    "- Using probability distribution to predict target values\n",
    "- Finding the centroid of data points to predict target values\n",
    "- Using gradient descent to optimize the model\n",
    "\n",
    "**Answer:** Combining multiple weak models to create a stronger model\n",
    "\n",
    "**Explanation:** The main idea behind the Random Forest Regressor is to combine the predictions of multiple weak models, typically decision trees, to create a stronger and more accurate model. This process is known as ensemble learning, where the weak models are combined to reduce bias and variance and improve the overall performance of the model.\n",
    "\n",
    "---\n",
    "\n",
    "**2. What is the criterion used for splitting nodes in a Random Forest Regressor?**\n",
    "\n",
    "- Gini impurity\n",
    "- Entropy\n",
    "- Mean squared error (MSE)\n",
    "- Mean absolute error (MAE)\n",
    "\n",
    "**Answer:** Mean squared error (MSE)\n",
    "\n",
    "**Explanation:** In Random Forest Regressor, the criterion used for splitting nodes in decision trees is typically the mean squared error (MSE). MSE measures the average squared difference between the predicted and actual values of the target variable. The node that results in the minimum MSE after splitting is selected as the splitting point.\n",
    "\n",
    "---\n",
    "\n",
    "**3. How are the decision trees combined in a Random Forest Regressor?**\n",
    "\n",
    "- By taking the average of their predictions\n",
    "- By taking the weighted average of their predictions\n",
    "- By selecting the most frequent prediction\n",
    "- By using a consensus vote\n",
    "\n",
    "**Answer:** By taking the average of their predictions\n",
    "\n",
    "**Explanation:** In a Random Forest Regressor, the predictions of all the individual decision trees in the forest are combined by taking the average of their predictions. This process is known as averaging or bagging, and it helps to reduce the variance and improve the stability of the model.\n",
    "\n",
    "---\n",
    "\n",
    "**4. What is the purpose of random feature selection in a Random Forest Regressor?**\n",
    "\n",
    "- To improve the model's accuracy\n",
    "- To reduce the model's complexity\n",
    "- To avoid overfitting\n",
    "- To speed up the training process\n",
    "\n",
    "**Answer:** To avoid overfitting\n",
    "\n",
    "**Explanation:** Random feature selection is a technique used in Random Forest Regressor to avoid overfitting. It involves randomly selecting a subset of features at each split during the construction of decision trees. This helps to introduce diversity in the trees and reduce the correlation between them, which in turn reduces overfitting and improves the generalization performance of the model.\n",
    "\n",
    "---\n",
    "\n",
    "**5. What is the default number of trees in a Random Forest Regressor in scikit-learn library in Python?**\n",
    "\n",
    "- 10\n",
    "- 50\n",
    "- 100\n",
    "- 500\n",
    "\n",
    "**Answer:** 100\n",
    "\n",
    "**Explanation:** The default number of trees in a Random Forest Regressor in scikit-learn library in Python is 100. However, this can be changed by specifying the \"n_estimators\" parameter during the model initialization. Increasing the number of trees may improve the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

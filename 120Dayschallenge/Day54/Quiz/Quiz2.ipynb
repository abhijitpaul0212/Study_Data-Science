{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the key concept behind Gradient Boosting?\n",
    "\n",
    "- Ensemble learning\n",
    "- Feature engineering\n",
    "- Gradient descent\n",
    "- Hyperparameter tuning\n",
    "\n",
    "**Answer:** Gradient descent\n",
    "\n",
    "**Explanation:** Gradient Boosting involves the iterative training of weak learners (often decision trees) where each subsequent learner corrects the errors made by the previous ones using gradient descent optimization.\n",
    "\n",
    "2. Which of the following is NOT a loss function used in Gradient Boosting?\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Cross-Entropy\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "**Answer:** Support Vector Machine (SVM)\n",
    "\n",
    "**Explanation:** SVM is a separate machine learning algorithm. Common loss functions used in Gradient Boosting include MSE, Cross-Entropy, and MAPE, which measure the difference between predicted and actual values.\n",
    "\n",
    "3. What is the purpose of regularization in Gradient Boosting?\n",
    "\n",
    "- To prevent overfitting\n",
    "- To increase model complexity\n",
    "- To reduce training time\n",
    "- To improve model interpretability\n",
    "\n",
    "**Answer:** To prevent overfitting\n",
    "\n",
    "**Explanation:** Regularization techniques such as L1 and L2 regularization are used in Gradient Boosting to prevent overfitting by constraining the model's complexity.\n",
    "\n",
    "4. What is the role of learning rate in Gradient Boosting?\n",
    "\n",
    "- It determines the number of boosting iterations\n",
    "- It controls the depth of decision trees\n",
    "- It adjusts the weights of misclassified samples\n",
    "- It scales the contribution of each tree to the ensemble\n",
    "\n",
    "**Answer:** It scales the contribution of each tree to the ensemble\n",
    "\n",
    "**Explanation:** The learning rate controls the impact of each tree on the final ensemble, with a smaller rate making the model more conservative and a larger rate making it more aggressive.\n",
    "\n",
    "5. What is the main advantage of Gradient Boosting over other machine learning algorithms?\n",
    "\n",
    "- Ability to handle missing data\n",
    "- Faster training time\n",
    "- Higher accuracy\n",
    "- Simpler implementation\n",
    "\n",
    "**Answer:** Higher accuracy\n",
    "\n",
    "**Explanation:** Gradient Boosting often yields higher accuracy compared to other algorithms due to its ability to capture complex relationships in the data through ensemble learning.\n",
    "\n",
    "6. What is the effect of increasing the number of boosting iterations in Gradient Boosting?\n",
    "\n",
    "- Increases the model complexity\n",
    "- Decreases the model complexity\n",
    "- Has no effect on the model complexity\n",
    "- Improves model interpretability\n",
    "\n",
    "**Answer:** Increases the model complexity\n",
    "\n",
    "**Explanation:** Increasing the number of boosting iterations results in a more complex model as more weak learners are added to the ensemble, potentially leading to overfitting if not controlled properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

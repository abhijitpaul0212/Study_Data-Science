{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the Curse of Dimensionality?**\n",
    "\n",
    "   a) A curse that affects only computer systems\n",
    "   \n",
    "   b) A phenomenon where the performance of machine learning algorithms deteriorates as the number of features or dimensions increases\n",
    "   \n",
    "   c) A type of curse used in voodoo rituals\n",
    "   \n",
    "   d) A term used to describe the increased complexity of a problem with high dimensionality\n",
    "\n",
    "**Answer:** \n",
    "b) A phenomenon where the performance of machine learning algorithms deteriorates as the number of features or dimensions increases\n",
    "\n",
    "**Explanation:** \n",
    "The Curse of Dimensionality is a phenomenon in which the performance of machine learning algorithms deteriorates as the number of features or dimensions increases. This occurs because the number of training examples required to accurately represent the space grows exponentially with the number of dimensions.\n",
    "\n",
    "**2. Which of the following is a common technique used for dimensionality reduction?**\n",
    "\n",
    "   a) Principal Component Analysis (PCA)\n",
    "   \n",
    "   b) Gaussian Naive Bayes (GNB)\n",
    "   \n",
    "   c) Linear Discriminant Analysis (LDA)\n",
    "   \n",
    "   d) K-Means Clustering\n",
    "\n",
    "**Answer:** \n",
    "a) Principal Component Analysis (PCA)\n",
    "\n",
    "**Explanation:** \n",
    "Principal Component Analysis (PCA) is a common technique used for dimensionality reduction. It involves transforming the original features into a new set of uncorrelated features called principal components, which retain most of the variance in the original data.\n",
    "\n",
    "**3. How does dimensionality reduction help with the Curse of Dimensionality?**\n",
    "\n",
    "   a) By increasing the number of features or dimensions\n",
    "   \n",
    "   b) By reducing the number of features or dimensions\n",
    "   \n",
    "   c) By increasing the number of training examples\n",
    "   \n",
    "   d) By using more complex algorithms\n",
    "\n",
    "**Answer:** \n",
    "b) By reducing the number of features or dimensions\n",
    "\n",
    "**Explanation:** \n",
    "Dimensionality reduction techniques reduce the number of features or dimensions in the data, which helps to mitigate the Curse of Dimensionality. By reducing the number of features, the number of training examples required to accurately represent the space decreases, making machine learning algorithms more effective.\n",
    "\n",
    "**4. Which of the following is a disadvantage of using dimensionality reduction techniques?**\n",
    "\n",
    "   a) They increase the interpretability of the data\n",
    "   \n",
    "   b) They can introduce noise and lose information\n",
    "   \n",
    "   c) They improve the performance of all machine learning algorithms\n",
    "   \n",
    "   d) They require more training examples\n",
    "\n",
    "**Answer:** \n",
    "b) They can introduce noise and lose information\n",
    "\n",
    "**Explanation:** \n",
    "One disadvantage of using dimensionality reduction techniques is that they can introduce noise and lose information. When the number of features is reduced, some information may be lost, and the remaining features may not be as informative. Additionally, some dimensionality reduction techniques can introduce noise into the data, which can affect the accuracy of machine learning models.\n",
    "\n",
    "**5. Which of the following is a type of dimensionality reduction technique that preserves the pairwise distances between data points?**\n",
    "\n",
    "   a) Linear Discriminant Analysis (LDA)\n",
    "   \n",
    "   b) t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "   \n",
    "   c) K-Means Clustering\n",
    "   \n",
    "   d) Random Forest Regression\n",
    "\n",
    "**Answer:** \n",
    "b) t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "**Explanation:** \n",
    "t-SNE is a type of dimensionality reduction technique that preserves the pairwise distances between data points. It is commonly used for visualizing high-dimensional data in two or three dimensions.\n",
    "\n",
    "**6. Which of the following statements is true about the Curse of Dimensionality?**\n",
    "\n",
    "   a) It affects only supervised machine learning algorithms\n",
    "   \n",
    "   b) It is caused by overfitting\n",
    "   \n",
    "   c) It can be solved by adding more features or dimensions\n",
    "   \n",
    "   d) It makes it harder to find meaningful patterns in high-dimensional data\n",
    "\n",
    "**Answer:** \n",
    "d) It makes it harder to find meaningful patterns in high-dimensional data\n",
    "\n",
    "**Explanation:** \n",
    "The Curse of Dimensionality makes it harder to find meaningful patterns in high-dimensional data. This occurs because the number of training examples required to accurately represent the space grows exponentially with the number of dimensions, making it difficult for machine learning algorithms to generalize well.\n",
    "\n",
    "**7. Which of the following is a technique used to handle missing values in high-dimensional data?**\n",
    "\n",
    "   a) Imputation\n",
    "   \n",
    "   b) Standardization\n",
    "   \n",
    "   c) Normalization\n",
    "   \n",
    "   d) Regularization\n",
    "\n",
    "**Answer:** \n",
    "a) Imputation\n",
    "\n",
    "**Explanation:** \n",
    "Imputation is a technique used to handle missing values in high-dimensional data. It involves filling in the missing values with estimated values based on the available data.\n",
    "\n",
    "**8. Which of the following is a dimensionality reduction technique that seeks to preserve the local structure of the data?**\n",
    "\n",
    "   a) Principal Component Analysis (PCA)\n",
    "   \n",
    "   b) Linear Discriminant Analysis (LDA)\n",
    "   \n",
    "   c) Isomap\n",
    "   \n",
    "   d) K-Means Clustering\n",
    "\n",
    "**Answer:** \n",
    "c) Isomap\n",
    "\n",
    "**Explanation:** \n",
    "Isomap is a dimensionality reduction technique that seeks to preserve the local structure of the data. It creates a low-dimensional representation of the data that preserves the pairwise geodesic distances between data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
